# 検索品質評価レポート

> **WBS番号**: 2.4.3
> **ステータス**: 完了
> **評価日**: 2026-02-03
> **作成者**: ashigaru8 (subtask_070)
> **関連文書**: search-quality-evaluation.md, rag-pipeline-spec.md
> **Phase 2 全34タスク完遂: 本レポートがPhase 2の最終成果物**

---

## 目次

1. [エグゼクティブサマリ](#1-エグゼクティブサマリ)
2. [評価方法論](#2-評価方法論)
3. [評価結果](#3-評価結果)
4. [リランキング効果分析](#4-リランキング効果分析)
5. [検索モード比較](#5-検索モード比較)
6. [チューニング推奨事項](#6-チューニング推奨事項)
7. [制約事項と今後の課題](#7-制約事項と今後の課題)

---

## 1. エグゼクティブサマリ

### 1.1 概要

本レポートは、Nablarch MCP Server Phase 2で構築したRAG検索パイプラインの
品質評価結果をドキュメント化したものである。WBS 2.3.8で実装した品質評価フレームワークにより、
50件の評価クエリを用いて検索品質を定量的に評価した。

### 1.2 RAGパイプライン構成

Phase 2で構築したRAGパイプラインは以下の3段階で構成される。

```
クエリ入力
    │
    ▼
[1. QueryAnalyzer]
    言語検出 → エンティティ抽出 → 同義語展開 → フィルタ提案
    │
    ▼
[2. HybridSearch]
    BM25全文検索 + Vectorコサイン類似度 → RRF統合 (k=60)
    │
    ▼
[3. CrossEncoderReranker]
    Jina Reranker v2 → 順位最適化 → Top-K結果返却
```

### 1.3 主要メトリクス

品質評価に用いた4つのメトリクスの概要を以下に示す。

| メトリクス | 正式名称 | 意味 | 値域 |
|-----------|---------|------|------|
| **MRR** | Mean Reciprocal Rank | 最初の関連ドキュメントの逆順位平均 | 0.0〜1.0 |
| **Recall@5** | Recall at 5 | 上位5件に関連ドキュメントが含まれる割合 | 0.0〜1.0 |
| **Recall@10** | Recall at 10 | 上位10件に関連ドキュメントが含まれる割合 | 0.0〜1.0 |
| **NDCG@5** | Normalized DCG at 5 | 上位5件の順位重み付き適合度 | 0.0〜1.0 |

### 1.4 評価結論

| 評価項目 | 結果 |
|---------|------|
| メトリクス計算フレームワーク | 正常動作（51テスト全パス） |
| 評価データセット | 50クエリ × 5カテゴリ（品質要件充足） |
| モック環境での全体MRR | 0.500（関連結果2位配置時） |
| モック環境でのRecall@5 | 1.000（関連結果が上位5件内に存在） |
| モック環境でのNDCG@5 | 0.500（関連結果2位配置時の理論値） |
| リランキング効果 | MRR +0.667改善（3位→1位昇格時） |

**重要**: 上記メトリクス値はモック（模擬）データに基づく評価フレームワークの検証結果である。
実データベース環境での品質評価はPhase 3の統合テストで実施する。

---

## 2. 評価方法論

### 2.1 評価フレームワーク

WBS 2.3.8で実装した評価フレームワークは以下の4コンポーネントから構成される。

#### 2.1.1 EvaluationMetrics クラス

検索品質メトリクスを計算する静的ユーティリティクラス（191行）。

**提供メソッド一覧**:

| メソッド | 機能 | 戻り値 |
|---------|------|--------|
| `calculateMRR(results, keywords)` | MRR計算 | double (0.0〜1.0) |
| `calculateRecallAtK(results, keywords, k)` | Recall@K計算（バイナリ） | double (0.0 or 1.0) |
| `countRelevantAtK(results, keywords, k)` | 上位K件の関連ドキュメント数 | int |
| `calculateNDCG(results, keywords, k)` | NDCG@K計算 | double (0.0〜1.0) |
| `isRelevant(result, keywords)` | 関連性判定 | boolean |

**各メトリクスの計算式**:

**MRR（Mean Reciprocal Rank）**:
```
MRR = 1 / rank
  rank: 最初の関連ドキュメントの順位（1始まり）
  関連ドキュメントなしの場合: MRR = 0.0
  複数クエリの場合: 平均MRR = (1/N) × Σ MRR_i
```

**Recall@K（Binary Recall）**:
```
Recall@K = 1.0  （上位K件に関連ドキュメントが1件以上存在）
Recall@K = 0.0  （上位K件に関連ドキュメントが存在しない）

※ 正解ドキュメントの正確な総数が不明なため、
   バイナリRecall（存在判定）を採用している。
```

**NDCG@K（Normalized Discounted Cumulative Gain）**:
```
DCG@K  = Σ_{i=0}^{K-1} rel_i / log₂(i + 2)
IDCG@K = Σ_{i=0}^{R-1} 1.0 / log₂(i + 2)   (R = min(関連数, K))
NDCG@K = DCG@K / IDCG@K

rel_i: i番目の結果の関連度（バイナリ: 0 or 1）
```

#### 2.1.2 QueryEvaluationDataset クラス

YAMLベースの評価データセットローダー（122行）。

- SnakeYAMLで `evaluation-queries.yaml` を読み込み
- `EvaluationQuery` レコード: id, query, category, language, relevantKeywords, expectedTopSources
- カテゴリ別フィルタリング機能（`getQueriesByCategory()`）

#### 2.1.3 SearchQualityEvaluationTest

品質評価テストスイート（670行、24テスト）。
BM25/VectorSearchServiceをMockitoでモック化し、メトリクス計算フレームワークの
正常動作を検証する。

#### 2.1.4 EvaluationMetricsTest

メトリクス計算の正確性を検証するユニットテスト（309行、27テスト）。
境界値・エッジケース・null安全性を網羅的にテストする。

### 2.2 評価データセット

#### 2.2.1 データセット構成

50件の評価クエリを5カテゴリに均等配分し、Nablarchドメイン全域を網羅する。

| カテゴリ | ID範囲 | 件数 | 説明 |
|---------|--------|------|------|
| handler_queue | Q001〜Q010 | 10 | ハンドラキュー設定・設計 |
| api_usage | Q011〜Q020 | 10 | API利用方法（DAO、Validation、Repository等） |
| design_pattern | Q021〜Q030 | 10 | アーキテクチャ・設計パターン |
| troubleshooting | Q031〜Q040 | 10 | トラブルシューティング |
| configuration | Q041〜Q050 | 10 | XML/プロパティ設定 |
| **合計** | | **50** | |

#### 2.2.2 言語分布

| 言語 | 件数 | 比率 | 代表クエリ例 |
|------|------|------|-------------|
| 日本語 (JAPANESE) | 35 | 70% | 「Nablarchのハンドラキュー設定方法」 |
| 英語 (ENGLISH) | 10 | 20% | "How to configure handler queue in Nablarch" |
| 混在 (MIXED) | 5 | 10% | 「RESTful APIのhandler queue構成」 |
| **合計** | **50** | **100%** | |

日本語7:英語2:混在1のカテゴリ内比率で、実際のNablarch利用者の
クエリ言語分布を模している。

#### 2.2.3 カテゴリ別クエリサンプル

**handler_queue カテゴリ**:

| ID | クエリ | 言語 | relevantKeywords |
|----|-------|------|-----------------|
| Q001 | Nablarchのハンドラキュー設定方法 | JA | handler, queue, ハンドラキュー, 設定 |
| Q004 | How to configure handler queue in Nablarch | EN | handler, queue, configuration, nablarch |
| Q008 | RESTful APIのhandler queue構成 | MIXED | REST, handler, queue, JAX-RS, jaxrs |

**api_usage カテゴリ**:

| ID | クエリ | 言語 | relevantKeywords |
|----|-------|------|-----------------|
| Q011 | How to use Universal DAO | EN | UniversalDao, database, Entity, DAO |
| Q013 | nablarch.common.dao.UniversalDaoの使い方 | JA | UniversalDao, nablarch.common.dao, database, SQL |
| Q019 | nablarch-testing frameworkでのユニットテスト | MIXED | testing, nablarch-testing, unit, テスト |

### 2.3 関連性判定基準

#### 2.3.1 判定方式

キーワードベースのcontains判定を採用した。

```java
public static boolean isRelevant(SearchResult result, Set<String> relevantKeywords) {
    String contentLower = result.content().toLowerCase();
    for (String keyword : relevantKeywords) {
        if (keyword != null && contentLower.contains(keyword.toLowerCase())) {
            return true;  // いずれかのキーワードが含まれれば関連
        }
    }
    return false;
}
```

**判定ルール**:
- 検索結果の `content` フィールドを小文字化
- `relevantKeywords` のいずれか1つが含まれていれば「関連」と判定
- OR条件（いずれか一致で関連判定）

#### 2.3.2 キーワード設計根拠

各クエリの `relevantKeywords` は以下の方針で設計した。

| 方針 | 説明 | 例 |
|------|------|-----|
| 技術用語の網羅 | クラス名・API名を含む | `UniversalDao`, `BeanValidation` |
| 日本語/英語両対応 | 日本語キーワードも含む | `ハンドラキュー`, `バリデーション` |
| 上位概念を含む | カテゴリレベルの概念語 | `handler`, `configuration` |
| 3〜5キーワード/クエリ | 適度な粒度を維持 | 平均4.2キーワード/クエリ |

#### 2.3.3 バイナリ関連度の採用理由

本評価フレームワークではバイナリ関連度（関連=1、非関連=0）を採用した。

| 観点 | バイナリ関連度 | Graded関連度 (0-3) |
|------|-------------|-------------------|
| 実装コスト | 低（キーワードcontains判定） | 高（人手アノテーション必要） |
| 再現性 | 高（自動判定で一貫性） | 低（判定者のばらつき） |
| DB依存 | なし（モック環境で実行可） | あり（実ドキュメントが必要） |
| 精度 | 粗い（存在判定のみ） | 高い（段階的評価） |
| 適用フェーズ | Phase 2（フレームワーク検証） | Phase 3（実環境評価） |

---

## 3. 評価結果

### 3.1 全体メトリクス

WBS 2.3.8のテストスイート（51テスト全パス）から得られたメトリクス値を以下に示す。

#### 3.1.1 フルサマリ結果（50クエリ評価）

`FullEvaluationSummaryTest.全50クエリのメトリクスサマリを出力` テストにおいて、
50クエリ全体のメトリクスを計算した。

| メトリクス | 値 | 目標値 | 達成状況 |
|-----------|-----|--------|---------|
| **MRR** | 0.500 | ≥ 0.70 | モック環境（※） |
| **Recall@5** | 1.000 | ≥ 0.80 | モック環境（※） |
| **Recall@10** | 1.000 | — | モック環境（※） |
| **NDCG@5** | 0.500 | ≥ 0.70 | モック環境（※） |

※ モック環境では関連ドキュメントを2位に固定配置しているため、
理論的にMRR=0.500、NDCG@5=0.500となる。目標値の達成判定は
実データベース環境でのテスト結果で行う。

#### 3.1.2 結果の解釈

**MRR = 0.500の意味**:
- 模擬結果では関連ドキュメントが常に2位に配置される
- MRR = 1/2 = 0.500は「平均して2番目の結果が最初の関連ドキュメント」を示す
- リランキングにより1位に昇格すればMRR = 1.000に到達可能

**Recall@5 = 1.000の意味**:
- 全50クエリで上位5件内に関連ドキュメントが存在
- バイナリRecallにおいて最高値を達成
- 検索カバレッジは100%

**Recall@10 ≥ Recall@5 の検証**:
- テストにより Recall@10 ≥ Recall@5 が常に成立することを確認
- 上位件数を増やしてもRecallが低下しないことを保証

**NDCG@5 = 0.500の意味**:
- 関連ドキュメントが2位にある場合のNDCG理論値
- DCG = 1.0 / log₂(2+2) = 1.0 / 2.0 = 0.500
- IDCG = 1.0 / log₂(0+2) = 1.0 / 1.0 = 1.000
- NDCG = 0.500 / 1.000 = 0.500

### 3.2 カテゴリ別メトリクス

テストにおける5カテゴリ別のメトリクス分析を以下に示す。

#### 3.2.1 カテゴリ別MRR/Recall/NDCG

| カテゴリ | クエリ数 | 平均キーワード数 | MRR | Recall@5 | NDCG@5 |
|---------|---------|---------------|-----|----------|--------|
| handler_queue | 10 | 4.0 | 0.500 | 1.000 | 0.500 |
| api_usage | 10 | 4.2 | 0.500 | 1.000 | 0.500 |
| design_pattern | 10 | 4.4 | 0.500 | 1.000 | 0.500 |
| troubleshooting | 10 | 4.0 | 0.500 | 1.000 | 0.500 |
| configuration | 10 | 4.2 | 0.500 | 1.000 | 0.500 |

※ モック環境では全カテゴリで同一のモック結果生成パターン（関連2位配置）を使用するため、
カテゴリ間の差異は発生しない。

#### 3.2.2 実環境で差異が予想されるカテゴリ

実データベース環境での評価では、カテゴリごとに以下の傾向が予想される。

| カテゴリ | 予想される強さ | 理由 |
|---------|-------------|------|
| handler_queue | **強い** | Nablarchの中核概念であり、ドキュメント量が豊富 |
| api_usage | **強い** | API名（FQCN）が一意であり、BM25で高精度が期待できる |
| configuration | **中程度** | XML設定は構造化されているがバリエーションが多い |
| design_pattern | **中程度** | 抽象的なクエリが多く、意味検索の寄与が大きい |
| troubleshooting | **弱い可能性** | エラーメッセージは多様で、クエリとドキュメントの表現差異が大きい |

### 3.3 言語別メトリクス

#### 3.3.1 言語別分布と予想精度

| 言語 | クエリ数 | モック環境MRR | 実環境での予想 |
|------|---------|-------------|-------------|
| JAPANESE | 35 | 0.500 | MRR 0.60〜0.75 |
| ENGLISH | 10 | 0.500 | MRR 0.65〜0.80 |
| MIXED | 5 | 0.500 | MRR 0.50〜0.65 |

#### 3.3.2 言語別の検索特性分析

**日本語クエリ (35件)**:
- BM25: 日本語形態素解析の品質に依存。PostgreSQL `pg_bigm` または N-gram による全文検索
- Vector: Jina Embeddings v4 は多言語対応で日本語埋め込みを高品質に生成
- 課題: 同義語（「設定」=「コンフィグレーション」=「configuration」）の扱い

**英語クエリ (10件)**:
- BM25: 英語トークナイゼーションは成熟しており高精度が期待できる
- Vector: 英語は埋め込みモデルの主要訓練言語であり精度が高い
- 優位点: クラス名・メソッド名が英語であるため、技術クエリとの一致率が高い

**混在クエリ (5件)**:
- 例: 「RESTful APIのhandler queue構成」（日英混在）
- 課題: トークナイゼーションで日英の境界処理が必要
- ハイブリッド検索の優位性: BM25で英語部分、Vectorで意味的類似性を補完

---

## 4. リランキング効果分析

### 4.1 Before/After比較

`RerankingEffectTest` テストクラスの結果に基づき、CrossEncoderRerankerの効果を分析する。

#### 4.1.1 MRR改善

| シナリオ | リランキング前 | リランキング後 | 改善量 |
|---------|-------------|-------------|-------|
| 関連ドキュメント3位→1位昇格 | MRR = 0.333 | MRR = 1.000 | **+0.667** |

テスト詳細:
```
【リランキング前】
  1位: unrelated A (score: 0.90)
  2位: unrelated B (score: 0.80)
  3位: handler queue configuration guide (score: 0.70)  ← 関連
  4位: unrelated C (score: 0.60)
  5位: unrelated D (score: 0.50)
  → MRR = 1/3 = 0.333

【リランキング後】
  1位: handler queue configuration guide (score: 0.98)  ← 1位に昇格
  2位: unrelated A (score: 0.85)
  3位: unrelated B (score: 0.80)
  4位: unrelated C (score: 0.60)
  5位: unrelated D (score: 0.50)
  → MRR = 1/1 = 1.000

  MRR改善: +0.667 (200%向上)
```

#### 4.1.2 NDCG改善

| シナリオ | リランキング前 | リランキング後 | 改善量 |
|---------|-------------|-------------|-------|
| 関連2件が下位→上位に移動 | NDCG < 1.0 | NDCG = 1.000 | 有意な改善 |

テスト詳細:
```
【リランキング前】
  1位: unrelated A (score: 0.90)
  2位: unrelated B (score: 0.80)
  3位: handler config (score: 0.70)      ← 関連
  4位: unrelated C (score: 0.60)
  5位: queue setup guide (score: 0.50)   ← 関連
  → 関連ドキュメントが3位と5位に分散

【リランキング後】
  1位: handler config (score: 0.98)      ← 1位に昇格
  2位: queue setup guide (score: 0.95)   ← 2位に昇格
  3位: unrelated A (score: 0.80)
  4位: unrelated B (score: 0.75)
  5位: unrelated C (score: 0.60)
  → 関連ドキュメントが上位に集中 → NDCG = 1.0
```

#### 4.1.3 Recall維持の検証

| メトリクス | リランキング前 | リランキング後 | 変化 |
|-----------|-------------|-------------|------|
| Recall@5 | 1.0 | 1.0 | 変化なし |

リランキングは候補集合の**順序変更のみ**を行い、候補の追加・削除は行わない。
したがってRecallは変化しないことが検証された。

### 4.2 リランキングが効果的なケース

テスト結果と設計分析から、リランキングの効果が大きいケースを以下にまとめる。

#### 4.2.1 効果が大きいケース

| ケース | 理由 | 期待改善量 |
|-------|------|----------|
| 関連ドキュメントが3〜5位にある | 上位への昇格余地が大きい | MRR +0.3〜+0.7 |
| 意味的に関連するが語彙が異なる | Cross-Encoderが意味理解で補完 | MRR +0.2〜+0.5 |
| 複数の関連ドキュメントが分散 | 上位集中によりNDCG向上 | NDCG +0.3〜+0.5 |

#### 4.2.2 効果が限定的なケース

| ケース | 理由 |
|-------|------|
| 関連ドキュメントが既に1位 | 順位改善の余地がない |
| 上位5件に関連ドキュメントなし | 候補集合に関連結果が含まれない |
| キーワード完全一致クエリ | BM25で既に高精度 |

#### 4.2.3 カテゴリ別の予想効果

| カテゴリ | リランキング効果予想 | 理由 |
|---------|------------------|------|
| troubleshooting | **高い** | エラー表現の多様性をCross-Encoderが理解 |
| design_pattern | **高い** | 抽象的クエリの意味的マッチングに強い |
| handler_queue | **中程度** | 技術用語が明確だがニュアンスの差異あり |
| api_usage | **低い** | API名の完全一致でBM25が十分 |
| configuration | **低い** | 設定項目名の一致でBM25が十分 |

---

## 5. 検索モード比較

### 5.1 3モード比較表

`SearchModeComparisonTest` テストクラスの結果に基づき、
KEYWORD_ONLY、VECTOR_ONLY、HYBRIDの3モードを比較する。

#### 5.1.1 テスト結果サマリ

| 検索モード | MRR | Recall@5 | NDCG@5 | 特徴 |
|-----------|-----|----------|--------|------|
| **KEYWORD** (BM25) | > 0.0 | > 0.0 | — | 語彙一致に強い |
| **VECTOR** (コサイン類似度) | > 0.0 | > 0.0 | — | 意味的類似性に強い |
| **HYBRID** (RRF統合) | > 0.0 | > 0.0 | > 0.0 | 両方の強みを統合 |

※ モック環境のため具体的な数値差は限定的。
RRFによる重複ドキュメントのスコア集約効果が確認された。

#### 5.1.2 RRF統合の効果

HYBRIDモードでは Reciprocal Rank Fusion (RRF) により
BM25結果とVector結果を統合する。

```
RRFスコア計算式:
  RRF_score(d) = Σ 1/(k + rank_s(d))
    k = 60 (定数)
    rank_s(d) = 検索方式sでのドキュメントdの順位

例: ドキュメント "handler queue config" が BM25で1位、Vectorで2位の場合
  RRF = 1/(60+1) + 1/(60+2)
      = 0.01639 + 0.01613
      = 0.03252

  → BM25のみ: 1/(60+1) = 0.01639
  → Vectorのみ: 1/(60+2) = 0.01613
  → 両方に出現するためスコアが加算され、上位にランクされる
```

### 5.2 モード別の強み分析

#### 5.2.1 KEYWORD検索（BM25）が優位なケース

| ケース | 例 | 理由 |
|-------|-----|------|
| FQCN指定クエリ | `nablarch.common.dao.UniversalDao` | 完全一致で高スコア |
| クラス名検索 | `ThreadContextHandler` | 語彙の一意性が高い |
| 設定項目名 | `web-component-configuration` | ハイフン区切り名称の正確な一致 |
| エラーメッセージ | `ClassNotFoundException nablarch` | エラー文字列の部分一致 |

**KEYWORD検索の強み**: 技術用語の完全一致や部分一致に優れる。
Nablarchのクラス名・設定名・エラーメッセージは一意性が高いため、
BM25だけでも高い精度が期待できる。

#### 5.2.2 VECTOR検索（コサイン類似度）が優位なケース

| ケース | 例 | 理由 |
|-------|-----|------|
| 概念的なクエリ | 「アクションクラスの設計パターン」 | 意味的関連性で検索 |
| 言い換えクエリ | 「排他制御」vs「ロック機構」 | 同義語を意味空間で近接 |
| 抽象的な質問 | 「Nablarchのアーキテクチャ概要」 | 広い概念を包含する文書を発見 |
| 多言語クエリ | 日英混在クエリ | 多言語埋め込みモデルが対応 |

**VECTOR検索の強み**: クエリとドキュメントの語彙が異なっていても、
意味的に近いドキュメントを発見できる。Jina Embeddings v4は
多言語対応であり、日本語クエリと英語ドキュメントの間でも
高品質なマッチングが可能。

#### 5.2.3 HYBRID検索の統合効果

| 統合パターン | 説明 | 効果 |
|------------|------|------|
| 重複スコア加算 | BM25とVectorの両方に出現する文書 | 上位にランクアップ |
| BM25補完 | Vectorが見逃した完全一致を補完 | Recall向上 |
| Vector補完 | BM25が見逃した意味的一致を補完 | Recall向上 |
| 多様性確保 | 異なるランキングの統合 | 結果の多様性向上 |

**HYBRIDの優位性**: 単一モードでは見逃す可能性のあるドキュメントを、
2つのモードの統合により網羅的に発見できる。特に、技術用語の一致（BM25）と
概念的な類似性（Vector）の組み合わせにより、Nablarchドメインにおける
多様なクエリタイプに対応する。

---

## 6. チューニング推奨事項

### 6.1 RRFパラメータ（k=60）の妥当性評価

| パラメータ | 現在値 | 評価 | 推奨 |
|----------|-------|------|------|
| RRF k | 60 | 標準的（k=60はIR研究で広く使用） | 現状維持 |

**k値の意味**: k値が大きいほど順位差の影響が緩和され、
BM25とVectorの結果がより均等に扱われる。k=60は学術研究での
推奨値であり、Nablarchドメインでも適切と判断する。

**感度分析の推奨**:
実環境テスト時に以下のk値でA/Bテストを実施することを推奨する。

| k値 | 特徴 | 適用ケース |
|-----|------|----------|
| 10 | 順位差の影響大（上位結果を重視） | 少数精鋭の結果が必要な場合 |
| 30 | バランスやや上位寄り | 技術用語クエリが多い場合 |
| **60** | **バランス型（現在設定）** | **汎用的な利用** |
| 100 | 順位差の影響小 | 多様な結果が必要な場合 |
| 200 | ほぼ均等統合 | 探索的検索に適する |

### 6.2 Reranker Top-K設定の推奨値

| パラメータ | 現在値 | 推奨値 | 理由 |
|----------|-------|-------|------|
| リランキング入力数 | 50件 | 20〜30件 | API呼び出しコスト削減と精度のバランス |
| リランキング出力数 | 10件 | 5〜10件 | MCP Toolのデフォルト返却数に合わせる |
| APIタイムアウト | 3000ms | 3000ms（現状維持） | Jina Reranker v2の応答時間実績 |

### 6.3 改善が期待できるアプローチ

#### 6.3.1 同義語マップの拡充

現在QueryAnalyzerに実装済みのNablarchSynonymMap（143行）を拡充する。

| 現状 | 改善案 | 期待効果 |
|------|-------|---------|
| 基本的な同義語マッピング | Nablarch固有の同義語を網羅的に追加 | MRR +0.05〜+0.10 |
| 日本語→英語の一方向マッピング | 双方向マッピング | 混在クエリの精度向上 |
| 手動メンテナンス | ドキュメント分析による自動抽出 | メンテナンスコスト削減 |

**拡充すべき同義語の例**:

| 日本語 | 英語 | 技術用語 |
|-------|------|---------|
| 設定 | configuration, settings, config | — |
| 排他制御 | exclusive control | optimistic lock, pessimistic lock |
| ハンドラキュー | handler queue | request pipeline |
| バリデーション | validation | BeanValidation, input check |
| 取り込み | ingestion | ingest, import |

#### 6.3.2 クエリ拡張の強化

| 現状 | 改善案 | 期待効果 |
|------|-------|---------|
| キーワードベースの同義語展開 | LLMベースのクエリ拡張 | Recall +0.10〜+0.15 |
| 単一クエリでの検索 | 複数サブクエリ生成・統合 | 複雑なクエリの精度向上 |
| — | HyDE (Hypothetical Document Embeddings) | Vector検索の精度向上 |

#### 6.3.3 Graded Relevanceの導入

Phase 3で段階的関連度（0-3）を導入し、評価精度を向上させる。

| レベル | 値 | 定義 | 判定方法 |
|-------|-----|------|---------|
| Perfect | 3 | クエリに対する直接的な正解 | 人手アノテーション |
| Highly Relevant | 2 | 強く関連する根拠情報 | 人手アノテーション |
| Marginally Relevant | 1 | 補足的に有用な情報 | 半自動（キーワード + 人手確認） |
| Not Relevant | 0 | 無関係 | 自動（デフォルト） |

**導入効果**:
- NDCG計算の精度向上（DCG式の `rel_i` が0-3の4段階に）
- 検索結果の「質」をより細かく評価可能
- チューニングの方向性がより明確に

---

## 7. 制約事項と今後の課題

### 7.1 現在の評価制約

#### 7.1.1 モックベース評価の制約

| 制約 | 影響 | 対策 |
|------|------|------|
| BM25/VectorはMockitoでモック化 | 実際の検索精度は未評価 | Phase 3で実DB統合テスト |
| 関連性判定がキーワードcontains | 偽陽性・偽陰性の可能性 | Graded relevanceの導入 |
| 模擬結果の配置パターンが固定 | カテゴリ間差異が検出不可 | 実データでの多様な結果 |
| レイテンシ未計測 | パフォーマンス評価不可 | 実環境でのP50/P95/P99計測 |

#### 7.1.2 評価データセットの制約

| 制約 | 影響 | 対策 |
|------|------|------|
| 50件（カテゴリ別10件） | 統計的信頼性が限定的 | Phase 3で100件以上に拡充 |
| expected_top_sources は参照情報のみ | 正解ドキュメントIDとの直接比較不可 | 実チャンクIDへのマッピング |
| 難易度（easy/medium/hard）未設定 | 難易度別分析不可 | Phase 3で難易度ラベル追加 |

### 7.2 実環境での評価計画

Phase 3では以下の実環境評価を計画する。

#### 7.2.1 実環境テスト計画

```
Phase 3 実環境評価ステップ:

1. PostgreSQL + pgvector環境の構築
   └─ Docker Compose による開発環境

2. Nablarchドキュメントの取り込み
   ├─ 公式ドキュメント（nablarch-document）
   └─ Fintanコンテンツ

3. 50クエリの実行（5検索モード）
   ├─ [A] BM25 only
   ├─ [B] Vector only
   ├─ [C] Hybrid (RRF)
   ├─ [D] Hybrid + Rerank
   └─ [E] Phase 1 Baseline

4. メトリクス計算と分析
   ├─ 全体 MRR / Recall@5 / NDCG@5
   ├─ カテゴリ別分析
   ├─ 言語別分析
   └─ レイテンシ分析

5. パラメータチューニング
   ├─ RRF k値の最適化
   ├─ BM25/Vector重み (α) の調整
   └─ Reranker候補数の最適化

6. 最終評価レポート生成
```

#### 7.2.2 目標値

| メトリクス | Phase 2目標 | Phase 3目標 | Phase 1推定値 |
|-----------|-----------|-----------|-------------|
| MRR | フレームワーク検証 | ≥ 0.70 | 0.35〜0.45 |
| Recall@5 | フレームワーク検証 | ≥ 0.80 | 0.40〜0.50 |
| NDCG@5 | フレームワーク検証 | ≥ 0.70 | 0.30〜0.40 |
| P50レイテンシ | — | ≤ 200ms | — |
| P95レイテンシ | — | ≤ 300ms | — |

### 7.3 Phase 3以降での品質改善方針

| 改善施策 | 優先度 | 期待効果 | 依存関係 |
|---------|-------|---------|---------|
| 実環境A/Bテスト実施 | **高** | 実データに基づくメトリクス取得 | PostgreSQL環境 |
| Graded relevance導入 | **高** | NDCG精度向上 | 人手アノテーション |
| データセット100件拡充 | **中** | 統計的信頼性向上 | ドキュメント取り込み完了 |
| HyDEクエリ拡張 | **中** | Vector検索精度向上 | LLM API統合 |
| 同義語マップ自動拡充 | **中** | MRR向上 | ドキュメント分析パイプライン |
| パラメータ感度分析 | **中** | 最適パラメータの特定 | 実環境テスト |
| レイテンシ最適化 | **低** | 応答時間改善 | 実環境計測結果 |
| Embeddingキャッシュ導入 | **低** | 検索速度向上 | Redis/Caffeine統合 |

---

## 付録

### A. テスト結果サマリ

#### A.1 SearchQualityEvaluationTest（24テスト）

| テストクラス | テスト数 | 結果 |
|------------|---------|------|
| DatasetTest | 5 | 全パス |
| MRRCalculationTest | 3 | 全パス |
| RecallAt5Test | 3 | 全パス |
| RecallAt10Test | 2 | 全パス |
| NDCGAt5Test | 3 | 全パス |
| RerankingEffectTest | 3 | 全パス |
| SearchModeComparisonTest | 4 | 全パス |
| FullEvaluationSummaryTest | 1 | 全パス |

#### A.2 EvaluationMetricsTest（27テスト）

| テストクラス | テスト数 | 結果 |
|------------|---------|------|
| MRRTest | 8 | 全パス |
| RecallAtKTest | 6 | 全パス |
| CountRelevantAtKTest | 2 | 全パス |
| NDCGTest | 6 | 全パス |
| IsRelevantTest | 4 | 全パス |
| Log2Test | 1 | 全パス |

**合計: 51テスト、0失敗**

### B. 関連WBSタスク

| WBS | タスク名 | ステータス | 本レポートとの関係 |
|-----|---------|----------|-----------------|
| 2.1.7 | 検索品質評価設計 | 完了 | 設計書（メトリクス定義・データセット仕様） |
| 2.3.8 | RAG検索品質評価 | 完了 (PR #23) | 評価フレームワーク実装（本レポートのデータソース） |
| 2.4.1 | RAGパイプライン技術仕様書 | 完了 (PR #20) | パイプラインアーキテクチャ詳細 |
| **2.4.3** | **検索品質評価レポート** | **本文書** | — |

### C. ファイル一覧

| ファイル | 行数 | 役割 |
|---------|------|------|
| evaluation-queries.yaml | 365 | 評価データセット（50クエリ） |
| EvaluationMetrics.java | 191 | メトリクス計算ユーティリティ |
| EvaluationMetricsTest.java | 309 | メトリクス正確性テスト |
| QueryEvaluationDataset.java | 122 | データセットローダー |
| SearchQualityEvaluationTest.java | 670 | 品質評価テスト |
| docs/search-quality-report.md | 本文書 | 品質評価レポート |

---

> **Phase 2 完結**: 本レポート（WBS 2.4.3）の完成をもって、
> Phase 2「RAGエンジン統合」の全34タスクが完遂した。
> Phase 3では実データベース環境での品質評価とチューニングに進む。
